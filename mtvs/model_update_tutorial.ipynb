{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61a4833",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기\n",
    "\n",
    "- 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022a5aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 참조 : https://www.tensorflow.org/tutorials/keras/regression?hl=ko\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080324/master/data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250a1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f057b25",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea9a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 0.7721 - accuracy: 0.7460\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.7800.hdf5\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.6038 - accuracy: 0.7516 - val_loss: 0.5024 - val_accuracy: 0.7800\n",
      "Epoch 2/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4825 - accuracy: 0.7820\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.8185.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4870 - accuracy: 0.7780 - val_loss: 0.4210 - val_accuracy: 0.8185\n",
      "Epoch 3/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4325 - accuracy: 0.7960\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.8462.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4006 - accuracy: 0.8116 - val_loss: 0.3413 - val_accuracy: 0.8462\n",
      "Epoch 4/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3250 - accuracy: 0.8580\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.8869.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8591 - val_loss: 0.2924 - val_accuracy: 0.8869\n",
      "Epoch 5/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3093 - accuracy: 0.8680\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.9123.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2893 - accuracy: 0.8884 - val_loss: 0.2616 - val_accuracy: 0.9123\n",
      "Epoch 6/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2981 - accuracy: 0.8960\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.9223.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2618 - accuracy: 0.9099 - val_loss: 0.2461 - val_accuracy: 0.9223\n",
      "Epoch 7/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2858 - accuracy: 0.9020\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9231.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2461 - accuracy: 0.9158 - val_loss: 0.2365 - val_accuracy: 0.9231\n",
      "Epoch 8/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2247 - accuracy: 0.9300\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9277.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9220 - val_loss: 0.2288 - val_accuracy: 0.9277\n",
      "Epoch 9/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2165 - accuracy: 0.9300\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9233 - val_loss: 0.2177 - val_accuracy: 0.9315\n",
      "Epoch 10/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1940 - accuracy: 0.9400\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2205 - accuracy: 0.9243 - val_loss: 0.2139 - val_accuracy: 0.9315\n",
      "Epoch 11/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2258 - accuracy: 0.9220\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2142 - accuracy: 0.9256 - val_loss: 0.2121 - val_accuracy: 0.9315\n",
      "Epoch 12/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1808 - accuracy: 0.9420\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9354.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.9266 - val_loss: 0.2128 - val_accuracy: 0.9354\n",
      "Epoch 13/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1713 - accuracy: 0.9420\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2078 - accuracy: 0.9284 - val_loss: 0.2061 - val_accuracy: 0.9315\n",
      "Epoch 14/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2231 - accuracy: 0.9220\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2059 - accuracy: 0.9297 - val_loss: 0.2044 - val_accuracy: 0.9315\n",
      "Epoch 15/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2260 - accuracy: 0.9260\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9315.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2056 - accuracy: 0.9279 - val_loss: 0.2207 - val_accuracy: 0.9315\n",
      "Epoch 16/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2268 - accuracy: 0.9180\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9362.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9299 - val_loss: 0.1988 - val_accuracy: 0.9362\n",
      "Epoch 17/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1732 - accuracy: 0.9440\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9338.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1984 - accuracy: 0.9305 - val_loss: 0.1983 - val_accuracy: 0.9338\n",
      "Epoch 18/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1800 - accuracy: 0.9400\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9354.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9315 - val_loss: 0.2033 - val_accuracy: 0.9354\n",
      "Epoch 19/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1806 - accuracy: 0.9440\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9369.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1927 - accuracy: 0.9320 - val_loss: 0.1923 - val_accuracy: 0.9369\n",
      "Epoch 20/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1822 - accuracy: 0.9380\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9362.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1896 - accuracy: 0.9348 - val_loss: 0.1924 - val_accuracy: 0.9362\n",
      "Epoch 21/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2106 - accuracy: 0.9300\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9369.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9325 - val_loss: 0.1950 - val_accuracy: 0.9369\n",
      "Epoch 22/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1729 - accuracy: 0.9480\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9385.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9341 - val_loss: 0.1870 - val_accuracy: 0.9385\n",
      "Epoch 23/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1777 - accuracy: 0.9500\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9392.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.9371 - val_loss: 0.1857 - val_accuracy: 0.9392\n",
      "Epoch 24/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1569 - accuracy: 0.9420\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9423.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1796 - accuracy: 0.9348 - val_loss: 0.1828 - val_accuracy: 0.9423\n",
      "Epoch 25/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1782 - accuracy: 0.9260\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9400.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1775 - accuracy: 0.9356 - val_loss: 0.1817 - val_accuracy: 0.9400\n",
      "Epoch 26/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1856 - accuracy: 0.9260\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9415.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9364 - val_loss: 0.1791 - val_accuracy: 0.9415\n",
      "Epoch 27/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1844 - accuracy: 0.9360\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9400.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9358 - val_loss: 0.1826 - val_accuracy: 0.9400\n",
      "Epoch 28/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1735 - accuracy: 0.9420\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9408.hdf5\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1736 - accuracy: 0.9364 - val_loss: 0.1782 - val_accuracy: 0.9408\n",
      "Epoch 29/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2001 - accuracy: 0.9220\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9423.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9384 - val_loss: 0.1746 - val_accuracy: 0.9423\n",
      "Epoch 30/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1710 - accuracy: 0.9400\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9431.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1694 - accuracy: 0.9389 - val_loss: 0.1720 - val_accuracy: 0.9431\n",
      "Epoch 31/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1692 - accuracy: 0.9400\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9431.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.9405 - val_loss: 0.1711 - val_accuracy: 0.9431\n",
      "Epoch 32/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1460 - accuracy: 0.9460\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9423.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9397 - val_loss: 0.1743 - val_accuracy: 0.9423\n",
      "Epoch 33/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1393 - accuracy: 0.9580\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9446.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9428 - val_loss: 0.1678 - val_accuracy: 0.9446\n",
      "Epoch 34/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1516 - accuracy: 0.9440\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9454.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9418 - val_loss: 0.1664 - val_accuracy: 0.9454\n",
      "Epoch 35/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1465 - accuracy: 0.9480\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9446.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1583 - accuracy: 0.9423 - val_loss: 0.1646 - val_accuracy: 0.9446\n",
      "Epoch 36/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1310 - accuracy: 0.9600\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9454.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9423 - val_loss: 0.1675 - val_accuracy: 0.9454\n",
      "Epoch 37/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9420\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9462.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9418 - val_loss: 0.1642 - val_accuracy: 0.9462\n",
      "Epoch 38/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1441 - accuracy: 0.9520\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9462.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1530 - accuracy: 0.9443 - val_loss: 0.1618 - val_accuracy: 0.9462\n",
      "Epoch 39/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1719 - accuracy: 0.9260\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9454.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9456 - val_loss: 0.1590 - val_accuracy: 0.9454\n",
      "Epoch 40/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1407 - accuracy: 0.9500\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9469.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.9451 - val_loss: 0.1613 - val_accuracy: 0.9469\n",
      "Epoch 41/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1769 - accuracy: 0.9460\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9477.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9474 - val_loss: 0.1563 - val_accuracy: 0.9477\n",
      "Epoch 42/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9500\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9477.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.9482 - val_loss: 0.1558 - val_accuracy: 0.9477\n",
      "Epoch 43/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1405 - accuracy: 0.9580\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9485.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9471 - val_loss: 0.1573 - val_accuracy: 0.9485\n",
      "Epoch 44/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1354 - accuracy: 0.9500\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9492.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9464 - val_loss: 0.1545 - val_accuracy: 0.9492\n",
      "Epoch 45/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1638 - accuracy: 0.9460\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9508.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9492 - val_loss: 0.1518 - val_accuracy: 0.9508\n",
      "Epoch 46/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1246 - accuracy: 0.9580\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9508.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9489 - val_loss: 0.1508 - val_accuracy: 0.9508\n",
      "Epoch 47/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1271 - accuracy: 0.9520\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9523.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1408 - accuracy: 0.9489 - val_loss: 0.1503 - val_accuracy: 0.9523\n",
      "Epoch 48/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1852 - accuracy: 0.9260\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9515.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9497 - val_loss: 0.1480 - val_accuracy: 0.9515\n",
      "Epoch 49/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1441 - accuracy: 0.9500\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9515.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9512 - val_loss: 0.1472 - val_accuracy: 0.9515\n",
      "Epoch 50/50\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1370 - accuracy: 0.9520\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9531.hdf5\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9510 - val_loss: 0.1466 - val_accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=1, callbacks=[checkpointer])\n",
    "#verbose 이차이가 다르다, 0, 1, 2 => 출력 결과물이 다르게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "240ecb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 641us/step - loss: 0.1317 - accuracy: 0.9515\n",
      "Test accuracy: 0.9515384435653687\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cf7a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 673us/step - loss: 0.5331 - accuracy: 0.7646\n",
      ".\\data\\model\\all\\01-0.7800.hdf5\n",
      "Test accuracy: 0.7646153569221497\n",
      "41/41 [==============================] - 0s 802us/step - loss: 0.0344 - accuracy: 0.9885\n",
      ".\\data\\model\\all\\01-0.9885.hdf5\n",
      "Test accuracy: 0.9884615540504456\n",
      "41/41 [==============================] - 0s 627us/step - loss: 0.4382 - accuracy: 0.8000\n",
      ".\\data\\model\\all\\02-0.8185.hdf5\n",
      "Test accuracy: 0.800000011920929\n",
      "41/41 [==============================] - 0s 623us/step - loss: 0.0303 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\02-0.9854.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 578us/step - loss: 0.3573 - accuracy: 0.8308\n",
      ".\\data\\model\\all\\03-0.8462.hdf5\n",
      "Test accuracy: 0.8307692408561707\n",
      "41/41 [==============================] - 0s 605us/step - loss: 0.0351 - accuracy: 0.9877\n",
      ".\\data\\model\\all\\03-0.9838.hdf5\n",
      "Test accuracy: 0.9876922965049744\n",
      "41/41 [==============================] - 0s 666us/step - loss: 0.3032 - accuracy: 0.8800\n",
      ".\\data\\model\\all\\04-0.8869.hdf5\n",
      "Test accuracy: 0.8799999952316284\n",
      "41/41 [==============================] - 0s 601us/step - loss: 0.0410 - accuracy: 0.9885\n",
      ".\\data\\model\\all\\04-0.9877.hdf5\n",
      "Test accuracy: 0.9884615540504456\n",
      "41/41 [==============================] - 0s 574us/step - loss: 0.2708 - accuracy: 0.9038\n",
      ".\\data\\model\\all\\05-0.9123.hdf5\n",
      "Test accuracy: 0.9038461446762085\n",
      "41/41 [==============================] - 0s 647us/step - loss: 0.0349 - accuracy: 0.9915\n",
      ".\\data\\model\\all\\05-0.9908.hdf5\n",
      "Test accuracy: 0.9915384650230408\n",
      "41/41 [==============================] - 0s 613us/step - loss: 0.2538 - accuracy: 0.9169\n",
      ".\\data\\model\\all\\06-0.9223.hdf5\n",
      "Test accuracy: 0.9169231057167053\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.0348 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\06-0.9892.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 639us/step - loss: 0.2387 - accuracy: 0.9177\n",
      ".\\data\\model\\all\\07-0.9231.hdf5\n",
      "Test accuracy: 0.9176923036575317\n",
      "41/41 [==============================] - 0s 635us/step - loss: 0.0305 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\07-0.9862.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 574us/step - loss: 0.2304 - accuracy: 0.9223\n",
      ".\\data\\model\\all\\08-0.9277.hdf5\n",
      "Test accuracy: 0.9223076701164246\n",
      "41/41 [==============================] - 0s 607us/step - loss: 0.0372 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\08-0.9892.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 538us/step - loss: 0.2225 - accuracy: 0.9231\n",
      ".\\data\\model\\all\\09-0.9315.hdf5\n",
      "Test accuracy: 0.9230769276618958\n",
      "41/41 [==============================] - 0s 577us/step - loss: 0.0354 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\09-0.9877.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 598us/step - loss: 0.2189 - accuracy: 0.9231\n",
      ".\\data\\model\\all\\10-0.9315.hdf5\n",
      "Test accuracy: 0.9230769276618958\n",
      "41/41 [==============================] - 0s 620us/step - loss: 0.0337 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\10-0.9892.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 606us/step - loss: 0.2158 - accuracy: 0.9246\n",
      ".\\data\\model\\all\\11-0.9315.hdf5\n",
      "Test accuracy: 0.9246153831481934\n",
      "41/41 [==============================] - 0s 626us/step - loss: 0.0326 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\11-0.9900.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 617us/step - loss: 0.2153 - accuracy: 0.9262\n",
      ".\\data\\model\\all\\12-0.9354.hdf5\n",
      "Test accuracy: 0.926153838634491\n",
      "41/41 [==============================] - 0s 622us/step - loss: 0.0372 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\12-0.9892.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 573us/step - loss: 0.2101 - accuracy: 0.9192\n",
      ".\\data\\model\\all\\13-0.9315.hdf5\n",
      "Test accuracy: 0.9192307591438293\n",
      "41/41 [==============================] - 0s 635us/step - loss: 0.0320 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\13-0.9877.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 618us/step - loss: 0.2080 - accuracy: 0.9208\n",
      ".\\data\\model\\all\\14-0.9315.hdf5\n",
      "Test accuracy: 0.920769214630127\n",
      "41/41 [==============================] - 0s 624us/step - loss: 0.0318 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\14-0.9854.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 573us/step - loss: 0.2196 - accuracy: 0.9254\n",
      ".\\data\\model\\all\\15-0.9315.hdf5\n",
      "Test accuracy: 0.9253846406936646\n",
      "41/41 [==============================] - 0s 626us/step - loss: 0.0317 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\15-0.9869.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 638us/step - loss: 0.2007 - accuracy: 0.9246\n",
      ".\\data\\model\\all\\16-0.9362.hdf5\n",
      "Test accuracy: 0.9246153831481934\n",
      "41/41 [==============================] - 0s 582us/step - loss: 0.0305 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\16-0.9869.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 601us/step - loss: 0.2014 - accuracy: 0.9246\n",
      ".\\data\\model\\all\\17-0.9338.hdf5\n",
      "Test accuracy: 0.9246153831481934\n",
      "41/41 [==============================] - 0s 599us/step - loss: 0.0315 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\17-0.9900.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 574us/step - loss: 0.2021 - accuracy: 0.9308\n",
      ".\\data\\model\\all\\18-0.9354.hdf5\n",
      "Test accuracy: 0.9307692050933838\n",
      "41/41 [==============================] - 0s 621us/step - loss: 0.0313 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\18-0.9877.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 599us/step - loss: 0.1930 - accuracy: 0.9285\n",
      ".\\data\\model\\all\\19-0.9369.hdf5\n",
      "Test accuracy: 0.9284615516662598\n",
      "41/41 [==============================] - 0s 575us/step - loss: 0.0386 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\19-0.9892.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 590us/step - loss: 0.1952 - accuracy: 0.9254\n",
      ".\\data\\model\\all\\20-0.9362.hdf5\n",
      "Test accuracy: 0.9253846406936646\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.0394 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\20-0.9892.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 625us/step - loss: 0.1932 - accuracy: 0.9323\n",
      ".\\data\\model\\all\\21-0.9369.hdf5\n",
      "Test accuracy: 0.9323077201843262\n",
      "41/41 [==============================] - 0s 576us/step - loss: 0.0369 - accuracy: 0.9885\n",
      ".\\data\\model\\all\\21-0.9885.hdf5\n",
      "Test accuracy: 0.9884615540504456\n",
      "41/41 [==============================] - 0s 626us/step - loss: 0.1861 - accuracy: 0.9315\n",
      ".\\data\\model\\all\\22-0.9385.hdf5\n",
      "Test accuracy: 0.931538462638855\n",
      "41/41 [==============================] - 0s 588us/step - loss: 0.0321 - accuracy: 0.9915\n",
      ".\\data\\model\\all\\22-0.9908.hdf5\n",
      "Test accuracy: 0.9915384650230408\n",
      "41/41 [==============================] - 0s 583us/step - loss: 0.1843 - accuracy: 0.9331\n",
      ".\\data\\model\\all\\23-0.9392.hdf5\n",
      "Test accuracy: 0.9330769181251526\n",
      "41/41 [==============================] - 0s 608us/step - loss: 0.0319 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\23-0.9900.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 573us/step - loss: 0.1816 - accuracy: 0.9346\n",
      ".\\data\\model\\all\\24-0.9423.hdf5\n",
      "Test accuracy: 0.9346153736114502\n",
      "41/41 [==============================] - 0s 596us/step - loss: 0.0304 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\24-0.9892.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 599us/step - loss: 0.1831 - accuracy: 0.9292\n",
      ".\\data\\model\\all\\25-0.9400.hdf5\n",
      "Test accuracy: 0.9292307496070862\n",
      "41/41 [==============================] - 0s 592us/step - loss: 0.0315 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\25-0.9900.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 599us/step - loss: 0.1792 - accuracy: 0.9323\n",
      ".\\data\\model\\all\\26-0.9415.hdf5\n",
      "Test accuracy: 0.9323077201843262\n",
      "41/41 [==============================] - 0s 651us/step - loss: 0.0303 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\26-0.9846.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 656us/step - loss: 0.1790 - accuracy: 0.9346\n",
      ".\\data\\model\\all\\27-0.9400.hdf5\n",
      "Test accuracy: 0.9346153736114502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 646us/step - loss: 0.0305 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\27-0.9869.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 621us/step - loss: 0.1750 - accuracy: 0.9354\n",
      ".\\data\\model\\all\\28-0.9408.hdf5\n",
      "Test accuracy: 0.9353846311569214\n",
      "41/41 [==============================] - 0s 638us/step - loss: 0.0313 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\28-0.9854.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 577us/step - loss: 0.1746 - accuracy: 0.9338\n",
      ".\\data\\model\\all\\29-0.9423.hdf5\n",
      "Test accuracy: 0.9338461756706238\n",
      "41/41 [==============================] - 0s 704us/step - loss: 0.0303 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\29-0.9869.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 605us/step - loss: 0.1707 - accuracy: 0.9362\n",
      ".\\data\\model\\all\\30-0.9431.hdf5\n",
      "Test accuracy: 0.9361538290977478\n",
      "41/41 [==============================] - 0s 621us/step - loss: 0.0301 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\30-0.9869.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 643us/step - loss: 0.1701 - accuracy: 0.9354\n",
      ".\\data\\model\\all\\31-0.9431.hdf5\n",
      "Test accuracy: 0.9353846311569214\n",
      "41/41 [==============================] - 0s 573us/step - loss: 0.0311 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\31-0.9862.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.1695 - accuracy: 0.9385\n",
      ".\\data\\model\\all\\32-0.9423.hdf5\n",
      "Test accuracy: 0.9384615421295166\n",
      "41/41 [==============================] - 0s 598us/step - loss: 0.0316 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\32-0.9900.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 620us/step - loss: 0.1644 - accuracy: 0.9385\n",
      ".\\data\\model\\all\\33-0.9446.hdf5\n",
      "Test accuracy: 0.9384615421295166\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.0371 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\33-0.9900.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.1626 - accuracy: 0.9392\n",
      ".\\data\\model\\all\\34-0.9454.hdf5\n",
      "Test accuracy: 0.939230740070343\n",
      "41/41 [==============================] - 0s 618us/step - loss: 0.0450 - accuracy: 0.9862\n",
      ".\\data\\model\\all\\34-0.9885.hdf5\n",
      "Test accuracy: 0.9861538410186768\n",
      "41/41 [==============================] - 0s 582us/step - loss: 0.1606 - accuracy: 0.9377\n",
      ".\\data\\model\\all\\35-0.9446.hdf5\n",
      "Test accuracy: 0.9376922845840454\n",
      "41/41 [==============================] - 0s 605us/step - loss: 0.0490 - accuracy: 0.9846\n",
      ".\\data\\model\\all\\35-0.9838.hdf5\n",
      "Test accuracy: 0.9846153855323792\n",
      "41/41 [==============================] - 0s 600us/step - loss: 0.1614 - accuracy: 0.9423\n",
      ".\\data\\model\\all\\36-0.9454.hdf5\n",
      "Test accuracy: 0.942307710647583\n",
      "41/41 [==============================] - 0s 595us/step - loss: 0.0624 - accuracy: 0.9762\n",
      ".\\data\\model\\all\\36-0.9792.hdf5\n",
      "Test accuracy: 0.9761538505554199\n",
      "41/41 [==============================] - 0s 580us/step - loss: 0.1586 - accuracy: 0.9415\n",
      ".\\data\\model\\all\\37-0.9462.hdf5\n",
      "Test accuracy: 0.9415384531021118\n",
      "41/41 [==============================] - 0s 571us/step - loss: 0.0328 - accuracy: 0.9915\n",
      ".\\data\\model\\all\\37-0.9908.hdf5\n",
      "Test accuracy: 0.9915384650230408\n",
      "41/41 [==============================] - 0s 596us/step - loss: 0.1590 - accuracy: 0.9408\n",
      ".\\data\\model\\all\\38-0.9462.hdf5\n",
      "Test accuracy: 0.9407692551612854\n",
      "41/41 [==============================] - 0s 550us/step - loss: 0.0306 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\38-0.9877.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 576us/step - loss: 0.1544 - accuracy: 0.9408\n",
      ".\\data\\model\\all\\39-0.9454.hdf5\n",
      "Test accuracy: 0.9407692551612854\n",
      "41/41 [==============================] - 0s 574us/step - loss: 0.0305 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\39-0.9846.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 596us/step - loss: 0.1545 - accuracy: 0.9438\n",
      ".\\data\\model\\all\\40-0.9469.hdf5\n",
      "Test accuracy: 0.9438461661338806\n",
      "41/41 [==============================] - 0s 602us/step - loss: 0.0307 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\40-0.9854.hdf5\n",
      "Test accuracy: 0.9900000095367432\n",
      "41/41 [==============================] - 0s 565us/step - loss: 0.1515 - accuracy: 0.9423\n",
      ".\\data\\model\\all\\41-0.9477.hdf5\n",
      "Test accuracy: 0.942307710647583\n",
      "41/41 [==============================] - 0s 642us/step - loss: 0.0305 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\41-0.9892.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 558us/step - loss: 0.1512 - accuracy: 0.9415\n",
      ".\\data\\model\\all\\42-0.9477.hdf5\n",
      "Test accuracy: 0.9415384531021118\n",
      "41/41 [==============================] - 0s 591us/step - loss: 0.0330 - accuracy: 0.9885\n",
      ".\\data\\model\\all\\42-0.9808.hdf5\n",
      "Test accuracy: 0.9884615540504456\n",
      "41/41 [==============================] - 0s 639us/step - loss: 0.1495 - accuracy: 0.9454\n",
      ".\\data\\model\\all\\43-0.9485.hdf5\n",
      "Test accuracy: 0.9453846216201782\n",
      "41/41 [==============================] - 0s 627us/step - loss: 0.0300 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\43-0.9892.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 604us/step - loss: 0.1470 - accuracy: 0.9454\n",
      ".\\data\\model\\all\\44-0.9492.hdf5\n",
      "Test accuracy: 0.9453846216201782\n",
      "41/41 [==============================] - 0s 702us/step - loss: 0.0301 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\44-0.9885.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 635us/step - loss: 0.1450 - accuracy: 0.9462\n",
      ".\\data\\model\\all\\45-0.9508.hdf5\n",
      "Test accuracy: 0.9461538195610046\n",
      "41/41 [==============================] - 0s 649us/step - loss: 0.0306 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\45-0.9854.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 639us/step - loss: 0.1438 - accuracy: 0.9454\n",
      ".\\data\\model\\all\\46-0.9508.hdf5\n",
      "Test accuracy: 0.9453846216201782\n",
      "41/41 [==============================] - 0s 608us/step - loss: 0.0304 - accuracy: 0.9892\n",
      ".\\data\\model\\all\\46-0.9862.hdf5\n",
      "Test accuracy: 0.989230751991272\n",
      "41/41 [==============================] - 0s 613us/step - loss: 0.1448 - accuracy: 0.9438\n",
      ".\\data\\model\\all\\47-0.9523.hdf5\n",
      "Test accuracy: 0.9438461661338806\n",
      "41/41 [==============================] - 0s 628us/step - loss: 0.0309 - accuracy: 0.9908\n",
      ".\\data\\model\\all\\47-0.9885.hdf5\n",
      "Test accuracy: 0.9907692074775696\n",
      "41/41 [==============================] - 0s 624us/step - loss: 0.1414 - accuracy: 0.9454\n",
      ".\\data\\model\\all\\48-0.9515.hdf5\n",
      "Test accuracy: 0.9453846216201782\n",
      "41/41 [==============================] - 0s 626us/step - loss: 0.0331 - accuracy: 0.9915\n",
      ".\\data\\model\\all\\48-0.9908.hdf5\n",
      "Test accuracy: 0.9915384650230408\n",
      "41/41 [==============================] - 0s 590us/step - loss: 0.1396 - accuracy: 0.9462\n",
      ".\\data\\model\\all\\49-0.9515.hdf5\n",
      "Test accuracy: 0.9461538195610046\n",
      "41/41 [==============================] - 0s 620us/step - loss: 0.0357 - accuracy: 0.9877\n",
      ".\\data\\model\\all\\49-0.9877.hdf5\n",
      "Test accuracy: 0.9876922965049744\n",
      "41/41 [==============================] - 0s 678us/step - loss: 0.1393 - accuracy: 0.9454\n",
      ".\\data\\model\\all\\50-0.9531.hdf5\n",
      "Test accuracy: 0.9453846216201782\n",
      "41/41 [==============================] - 0s 601us/step - loss: 0.0294 - accuracy: 0.9900\n",
      ".\\data\\model\\all\\50-0.9877.hdf5\n",
      "Test accuracy: 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "model_List = glob('.\\\\data\\\\model\\\\all\\\\*.hdf5')\n",
    "\n",
    "for i in model_List:\n",
    "    \n",
    "    loadModel = tf.keras.models.load_model(i)\n",
    "    score=loadModel.evaluate(X_test, y_test)\n",
    "    print(i)\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "\n",
    "# 테스트 90 이상 것만 뽑아서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d7667f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test.h5') # 학습된 모델을 저장 하는 방법 포멧은 h5로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb75948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2337</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.99215</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.39</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6061</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.038</td>\n",
       "      <td>28.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.98968</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.30</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.039</td>\n",
       "      <td>14.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.99420</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.34</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.027</td>\n",
       "      <td>35.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.99248</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>7.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.58</td>\n",
       "      <td>16.75</td>\n",
       "      <td>0.050</td>\n",
       "      <td>43.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.99990</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2      3      4     5      6        7     8     9     10  \\\n",
       "2337  7.1  0.21  0.31   3.80  0.021  40.0  142.0  0.99215  3.17  0.39  10.8   \n",
       "6061  7.1  0.42  0.20   2.80  0.038  28.0  109.0  0.98968  3.23  0.47  13.4   \n",
       "3500  7.4  0.21  0.30   7.90  0.039  14.0  118.0  0.99420  2.96  0.34  10.4   \n",
       "4051  7.3  0.19  0.27   1.60  0.027  35.0  136.0  0.99248  3.38  0.54  11.0   \n",
       "3777  7.6  0.32  0.58  16.75  0.050  43.0  163.0  0.99990  3.15  0.54   9.2   \n",
       "\n",
       "      11  \n",
       "2337   7  \n",
       "6061   6  \n",
       "3500   5  \n",
       "4051   7  \n",
       "3777   5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a091488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00014264]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:1])\n",
    "\n",
    "#데이터 디멘션이 다르다, 1차원과 2차원의 문제이기 때문에, 차원 잘 보기\n",
    "#예측값을 넣어주실땐 2차원으로 넣어줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f6fc6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 452us/step\n",
      "0.4 (array([False,  True]), array([  26, 1274], dtype=int64))\n",
      "41/41 [==============================] - 0s 522us/step\n",
      "0.5 (array([False,  True]), array([  18, 1282], dtype=int64))\n",
      "41/41 [==============================] - 0s 473us/step\n",
      "0.6 (array([False,  True]), array([  17, 1283], dtype=int64))\n",
      "41/41 [==============================] - 0s 450us/step\n",
      "0.7 (array([False,  True]), array([  13, 1287], dtype=int64))\n",
      "41/41 [==============================] - 0s 609us/step\n",
      "0.8 (array([False,  True]), array([  15, 1285], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#임계값을 바꿔가면서 적용해본다.\n",
    "\n",
    "valid =[0.4,0.5,0.6,0.7,0.8] #임계값을 바꿔가면서 예측을 맞췄는지 확인해본다.\n",
    "\n",
    "for x in valid:\n",
    "    result = model.predict(X_test)\n",
    "    result[result < x] = 0\n",
    "    result[result >= x] = 1\n",
    "    \n",
    "    print(x, np.unique(y_test==result[:,0], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441864af",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9393a6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47295b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.135033</td>\n",
       "      <td>0.950988</td>\n",
       "      <td>0.144893</td>\n",
       "      <td>0.952308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133887</td>\n",
       "      <td>0.952784</td>\n",
       "      <td>0.143780</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.133180</td>\n",
       "      <td>0.951245</td>\n",
       "      <td>0.142773</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.132143</td>\n",
       "      <td>0.952784</td>\n",
       "      <td>0.143705</td>\n",
       "      <td>0.954615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.142665</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.029641</td>\n",
       "      <td>0.992302</td>\n",
       "      <td>0.081082</td>\n",
       "      <td>0.983077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.032293</td>\n",
       "      <td>0.992045</td>\n",
       "      <td>0.073643</td>\n",
       "      <td>0.983846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.032022</td>\n",
       "      <td>0.991019</td>\n",
       "      <td>0.078309</td>\n",
       "      <td>0.982308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.030477</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.075859</td>\n",
       "      <td>0.982308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.032097</td>\n",
       "      <td>0.989992</td>\n",
       "      <td>0.083807</td>\n",
       "      <td>0.980769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.135033  0.950988  0.144893      0.952308\n",
       "1     0.133887  0.952784  0.143780      0.953846\n",
       "2     0.133180  0.951245  0.142773      0.953846\n",
       "3     0.132143  0.952784  0.143705      0.954615\n",
       "4     0.131333  0.953297  0.142665      0.953846\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.029641  0.992302  0.081082      0.983077\n",
       "1996  0.032293  0.992045  0.073643      0.983846\n",
       "1997  0.032022  0.991019  0.078309      0.982308\n",
       "1998  0.030477  0.991275  0.075859      0.982308\n",
       "1999  0.032097  0.989992  0.083807      0.980769\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef01ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5774c45",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단\n",
    "-기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac76966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/gilbutITbook/080324/master/data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b643a8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13.1152 - accuracy: 0.2471 - val_loss: 8.0808 - val_accuracy: 0.2485\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 4.8783 - accuracy: 0.2428 - val_loss: 1.2162 - val_accuracy: 0.2462\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8219 - accuracy: 0.4493 - val_loss: 0.4689 - val_accuracy: 0.7846\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4179 - accuracy: 0.7734 - val_loss: 0.3735 - val_accuracy: 0.7677\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.7655 - val_loss: 0.3644 - val_accuracy: 0.7700\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3783 - accuracy: 0.7762 - val_loss: 0.3531 - val_accuracy: 0.7938\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.8093 - val_loss: 0.3492 - val_accuracy: 0.8346\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3610 - accuracy: 0.8314 - val_loss: 0.3433 - val_accuracy: 0.8423\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3535 - accuracy: 0.8365 - val_loss: 0.3338 - val_accuracy: 0.8415\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.8389 - val_loss: 0.3266 - val_accuracy: 0.8469\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3395 - accuracy: 0.8437 - val_loss: 0.3197 - val_accuracy: 0.8592\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3322 - accuracy: 0.8514 - val_loss: 0.3123 - val_accuracy: 0.8708\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3249 - accuracy: 0.8619 - val_loss: 0.3041 - val_accuracy: 0.8769\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8704 - val_loss: 0.2956 - val_accuracy: 0.8892\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3099 - accuracy: 0.8758 - val_loss: 0.2873 - val_accuracy: 0.8977\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3024 - accuracy: 0.8825 - val_loss: 0.2797 - val_accuracy: 0.9031\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2953 - accuracy: 0.8858 - val_loss: 0.2717 - val_accuracy: 0.9054\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2880 - accuracy: 0.8881 - val_loss: 0.2651 - val_accuracy: 0.9092\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2805 - accuracy: 0.8920 - val_loss: 0.2581 - val_accuracy: 0.9100\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2741 - accuracy: 0.8958 - val_loss: 0.2516 - val_accuracy: 0.9115\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2671 - accuracy: 0.8981 - val_loss: 0.2465 - val_accuracy: 0.9162\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.9007 - val_loss: 0.2410 - val_accuracy: 0.9177\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2558 - accuracy: 0.9035 - val_loss: 0.2357 - val_accuracy: 0.9200\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2514 - accuracy: 0.9053 - val_loss: 0.2319 - val_accuracy: 0.9192\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2470 - accuracy: 0.9094 - val_loss: 0.2277 - val_accuracy: 0.9192\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2431 - accuracy: 0.9138 - val_loss: 0.2258 - val_accuracy: 0.9231\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2400 - accuracy: 0.9156 - val_loss: 0.2214 - val_accuracy: 0.9215\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2371 - accuracy: 0.9153 - val_loss: 0.2197 - val_accuracy: 0.9238\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2342 - accuracy: 0.9187 - val_loss: 0.2163 - val_accuracy: 0.9254\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2315 - accuracy: 0.9194 - val_loss: 0.2141 - val_accuracy: 0.9246\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2290 - accuracy: 0.9207 - val_loss: 0.2124 - val_accuracy: 0.9262\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9233 - val_loss: 0.2096 - val_accuracy: 0.9269\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2240 - accuracy: 0.9235 - val_loss: 0.2070 - val_accuracy: 0.9285\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2205 - accuracy: 0.9246 - val_loss: 0.2047 - val_accuracy: 0.9292\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9256 - val_loss: 0.1999 - val_accuracy: 0.9308\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.9274 - val_loss: 0.1946 - val_accuracy: 0.9308\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2049 - accuracy: 0.9279 - val_loss: 0.1901 - val_accuracy: 0.9300\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2002 - accuracy: 0.9281 - val_loss: 0.1873 - val_accuracy: 0.9354\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9323 - val_loss: 0.1857 - val_accuracy: 0.9346\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9310 - val_loss: 0.1839 - val_accuracy: 0.9369\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1916 - accuracy: 0.9325 - val_loss: 0.1821 - val_accuracy: 0.9377\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9338 - val_loss: 0.1801 - val_accuracy: 0.9392\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1882 - accuracy: 0.9341 - val_loss: 0.1783 - val_accuracy: 0.9400\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1872 - accuracy: 0.9343 - val_loss: 0.1771 - val_accuracy: 0.9392\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9364 - val_loss: 0.1759 - val_accuracy: 0.9385\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9358 - val_loss: 0.1749 - val_accuracy: 0.9377\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9371 - val_loss: 0.1742 - val_accuracy: 0.9385\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.9361 - val_loss: 0.1737 - val_accuracy: 0.9400\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9364 - val_loss: 0.1730 - val_accuracy: 0.9362\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9364 - val_loss: 0.1723 - val_accuracy: 0.9400\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9371 - val_loss: 0.1719 - val_accuracy: 0.9354\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9371 - val_loss: 0.1714 - val_accuracy: 0.9377\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9384 - val_loss: 0.1705 - val_accuracy: 0.9385\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9387 - val_loss: 0.1697 - val_accuracy: 0.9392\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9382 - val_loss: 0.1688 - val_accuracy: 0.9377\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1786 - accuracy: 0.9392 - val_loss: 0.1680 - val_accuracy: 0.9385\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1777 - accuracy: 0.9402 - val_loss: 0.1671 - val_accuracy: 0.9392\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1766 - accuracy: 0.9407 - val_loss: 0.1661 - val_accuracy: 0.9400\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1753 - accuracy: 0.9402 - val_loss: 0.1658 - val_accuracy: 0.9423\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9425 - val_loss: 0.1640 - val_accuracy: 0.9415\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1722 - accuracy: 0.9428 - val_loss: 0.1651 - val_accuracy: 0.9408\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1723 - accuracy: 0.9402 - val_loss: 0.1611 - val_accuracy: 0.9423\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.9433 - val_loss: 0.1607 - val_accuracy: 0.9446\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1686 - accuracy: 0.9438 - val_loss: 0.1599 - val_accuracy: 0.9446\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9423 - val_loss: 0.1576 - val_accuracy: 0.9438\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1664 - accuracy: 0.9451 - val_loss: 0.1566 - val_accuracy: 0.9446\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1653 - accuracy: 0.9433 - val_loss: 0.1548 - val_accuracy: 0.9431\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.9438 - val_loss: 0.1555 - val_accuracy: 0.9438\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9446 - val_loss: 0.1521 - val_accuracy: 0.9438\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9461 - val_loss: 0.1515 - val_accuracy: 0.9462\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.9464 - val_loss: 0.1524 - val_accuracy: 0.9454\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9451 - val_loss: 0.1510 - val_accuracy: 0.9462\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9466 - val_loss: 0.1518 - val_accuracy: 0.9462\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9466 - val_loss: 0.1460 - val_accuracy: 0.9492\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9464 - val_loss: 0.1512 - val_accuracy: 0.9469\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9464 - val_loss: 0.1443 - val_accuracy: 0.9477\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9464 - val_loss: 0.1427 - val_accuracy: 0.9500\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.9464 - val_loss: 0.1427 - val_accuracy: 0.9469\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1483 - accuracy: 0.9479 - val_loss: 0.1405 - val_accuracy: 0.9485\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9469 - val_loss: 0.1382 - val_accuracy: 0.9477\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1453 - accuracy: 0.9466 - val_loss: 0.1376 - val_accuracy: 0.9515\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1424 - accuracy: 0.9487 - val_loss: 0.1356 - val_accuracy: 0.9523\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9482 - val_loss: 0.1365 - val_accuracy: 0.9538\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9487 - val_loss: 0.1321 - val_accuracy: 0.9531\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9502 - val_loss: 0.1305 - val_accuracy: 0.9538\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9482 - val_loss: 0.1294 - val_accuracy: 0.9538\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1336 - accuracy: 0.9497 - val_loss: 0.1285 - val_accuracy: 0.9546\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9489 - val_loss: 0.1341 - val_accuracy: 0.9562\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1310 - accuracy: 0.9507 - val_loss: 0.1275 - val_accuracy: 0.9569\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9515 - val_loss: 0.1249 - val_accuracy: 0.9538\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9500 - val_loss: 0.1238 - val_accuracy: 0.9538\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9512 - val_loss: 0.1257 - val_accuracy: 0.9592\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1249 - accuracy: 0.9520 - val_loss: 0.1225 - val_accuracy: 0.9577\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9523 - val_loss: 0.1218 - val_accuracy: 0.9569\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9530 - val_loss: 0.1209 - val_accuracy: 0.9585\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9536 - val_loss: 0.1197 - val_accuracy: 0.9592\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9538 - val_loss: 0.1220 - val_accuracy: 0.9608\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9541 - val_loss: 0.1217 - val_accuracy: 0.9608\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9554 - val_loss: 0.1164 - val_accuracy: 0.9608\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1194 - accuracy: 0.9548 - val_loss: 0.1162 - val_accuracy: 0.9600\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9566 - val_loss: 0.1154 - val_accuracy: 0.9615\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.9561 - val_loss: 0.1142 - val_accuracy: 0.9615\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1163 - accuracy: 0.9582 - val_loss: 0.1139 - val_accuracy: 0.9592\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9574 - val_loss: 0.1137 - val_accuracy: 0.9569\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9571 - val_loss: 0.1118 - val_accuracy: 0.9615\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1139 - accuracy: 0.9569 - val_loss: 0.1150 - val_accuracy: 0.9615\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9579 - val_loss: 0.1146 - val_accuracy: 0.9631\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1151 - accuracy: 0.9566 - val_loss: 0.1095 - val_accuracy: 0.9631\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1125 - accuracy: 0.9610 - val_loss: 0.1089 - val_accuracy: 0.9623\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9595 - val_loss: 0.1092 - val_accuracy: 0.9600\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1144 - accuracy: 0.9579 - val_loss: 0.1076 - val_accuracy: 0.9631\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.9587 - val_loss: 0.1103 - val_accuracy: 0.9638\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1103 - accuracy: 0.9592 - val_loss: 0.1088 - val_accuracy: 0.9654\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9605 - val_loss: 0.1082 - val_accuracy: 0.9662\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9620 - val_loss: 0.1050 - val_accuracy: 0.9638\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9625 - val_loss: 0.1049 - val_accuracy: 0.9662\n",
      "Epoch 117/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.9610 - val_loss: 0.1097 - val_accuracy: 0.9662\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.9625 - val_loss: 0.1033 - val_accuracy: 0.9662\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9633 - val_loss: 0.1034 - val_accuracy: 0.9608\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9630 - val_loss: 0.1014 - val_accuracy: 0.9646\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1053 - accuracy: 0.9628 - val_loss: 0.1067 - val_accuracy: 0.9700\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9620 - val_loss: 0.1005 - val_accuracy: 0.9677\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1041 - accuracy: 0.9641 - val_loss: 0.1000 - val_accuracy: 0.9646\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9646 - val_loss: 0.0997 - val_accuracy: 0.9638\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1029 - accuracy: 0.9630 - val_loss: 0.1021 - val_accuracy: 0.9715\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9628 - val_loss: 0.0999 - val_accuracy: 0.9700\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1043 - accuracy: 0.9623 - val_loss: 0.0973 - val_accuracy: 0.9677\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1021 - accuracy: 0.9659 - val_loss: 0.0989 - val_accuracy: 0.9608\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.9654 - val_loss: 0.0967 - val_accuracy: 0.9646\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1010 - accuracy: 0.9630 - val_loss: 0.1012 - val_accuracy: 0.9731\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9638 - val_loss: 0.0961 - val_accuracy: 0.9723\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.9646 - val_loss: 0.0946 - val_accuracy: 0.9685\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0983 - accuracy: 0.9654 - val_loss: 0.0937 - val_accuracy: 0.9708\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9638 - val_loss: 0.0995 - val_accuracy: 0.9746\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9646 - val_loss: 0.0945 - val_accuracy: 0.9762\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0975 - accuracy: 0.9664 - val_loss: 0.0952 - val_accuracy: 0.9738\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9654 - val_loss: 0.0917 - val_accuracy: 0.9715\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9661 - val_loss: 0.0912 - val_accuracy: 0.9723\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9682 - val_loss: 0.0928 - val_accuracy: 0.9631\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0952 - accuracy: 0.9674 - val_loss: 0.0961 - val_accuracy: 0.9746\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0962 - accuracy: 0.9651 - val_loss: 0.0921 - val_accuracy: 0.9769\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0938 - accuracy: 0.9687 - val_loss: 0.0895 - val_accuracy: 0.9715\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9695 - val_loss: 0.0906 - val_accuracy: 0.9662\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.9661 - val_loss: 0.0889 - val_accuracy: 0.9762\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9682 - val_loss: 0.0921 - val_accuracy: 0.9762\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0922 - accuracy: 0.9692 - val_loss: 0.0880 - val_accuracy: 0.9746\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9700 - val_loss: 0.0876 - val_accuracy: 0.9777\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9674 - val_loss: 0.0952 - val_accuracy: 0.9746\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0927 - accuracy: 0.9687 - val_loss: 0.0864 - val_accuracy: 0.9715\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9705 - val_loss: 0.0859 - val_accuracy: 0.9738\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9695 - val_loss: 0.0855 - val_accuracy: 0.9777\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0893 - accuracy: 0.9697 - val_loss: 0.0877 - val_accuracy: 0.9785\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0881 - accuracy: 0.9715 - val_loss: 0.0852 - val_accuracy: 0.9715\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9695 - val_loss: 0.0855 - val_accuracy: 0.9785\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9695 - val_loss: 0.0839 - val_accuracy: 0.9785\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0873 - accuracy: 0.9720 - val_loss: 0.0841 - val_accuracy: 0.9785\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9705 - val_loss: 0.0827 - val_accuracy: 0.9762\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9715 - val_loss: 0.0829 - val_accuracy: 0.9777\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.9718 - val_loss: 0.0822 - val_accuracy: 0.9777\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0856 - accuracy: 0.9705 - val_loss: 0.0824 - val_accuracy: 0.9777\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9700 - val_loss: 0.0815 - val_accuracy: 0.9777\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9700 - val_loss: 0.0814 - val_accuracy: 0.9738\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0882 - accuracy: 0.9687 - val_loss: 0.0933 - val_accuracy: 0.9608\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0927 - accuracy: 0.9687 - val_loss: 0.0814 - val_accuracy: 0.9746\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9692 - val_loss: 0.0801 - val_accuracy: 0.9785\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0836 - accuracy: 0.9736 - val_loss: 0.0810 - val_accuracy: 0.9785\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9715 - val_loss: 0.0799 - val_accuracy: 0.9777\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9710 - val_loss: 0.0813 - val_accuracy: 0.9731\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9713 - val_loss: 0.0799 - val_accuracy: 0.9762\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9725 - val_loss: 0.0800 - val_accuracy: 0.9762\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9720 - val_loss: 0.0794 - val_accuracy: 0.9792\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9738 - val_loss: 0.0782 - val_accuracy: 0.9777\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9777\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9723 - val_loss: 0.0785 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0803 - accuracy: 0.9733 - val_loss: 0.0781 - val_accuracy: 0.9792\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9736 - val_loss: 0.0765 - val_accuracy: 0.9769\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9749 - val_loss: 0.0779 - val_accuracy: 0.9762\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.9728 - val_loss: 0.0758 - val_accuracy: 0.9762\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9728 - val_loss: 0.0752 - val_accuracy: 0.9777\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.9731 - val_loss: 0.0767 - val_accuracy: 0.9769\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9743 - val_loss: 0.0760 - val_accuracy: 0.9769\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9754 - val_loss: 0.0747 - val_accuracy: 0.9777\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9743 - val_loss: 0.0749 - val_accuracy: 0.9777\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.0775 - val_accuracy: 0.9769\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9736 - val_loss: 0.0781 - val_accuracy: 0.9777\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9746 - val_loss: 0.0735 - val_accuracy: 0.9777\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9754 - val_loss: 0.0728 - val_accuracy: 0.9800\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0755 - accuracy: 0.9761 - val_loss: 0.0725 - val_accuracy: 0.9808\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9751 - val_loss: 0.0721 - val_accuracy: 0.9800\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9754 - val_loss: 0.0727 - val_accuracy: 0.9785\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9749 - val_loss: 0.0719 - val_accuracy: 0.9785\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0751 - accuracy: 0.9754 - val_loss: 0.0711 - val_accuracy: 0.9792\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9774 - val_loss: 0.0733 - val_accuracy: 0.9777\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9769 - val_loss: 0.0715 - val_accuracy: 0.9785\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0720 - accuracy: 0.9772 - val_loss: 0.0718 - val_accuracy: 0.9762\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9764 - val_loss: 0.0705 - val_accuracy: 0.9785\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9772 - val_loss: 0.0701 - val_accuracy: 0.9792\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0740 - accuracy: 0.9769 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0714 - accuracy: 0.9777 - val_loss: 0.0694 - val_accuracy: 0.9800\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0706 - accuracy: 0.9774 - val_loss: 0.0688 - val_accuracy: 0.9792\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9782 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9784 - val_loss: 0.0682 - val_accuracy: 0.9800\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 641us/step - loss: 0.0708 - accuracy: 0.9782 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0709 - accuracy: 0.9769 - val_loss: 0.0690 - val_accuracy: 0.9792\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9766 - val_loss: 0.0760 - val_accuracy: 0.9769\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9772 - val_loss: 0.0717 - val_accuracy: 0.9785\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9756 - val_loss: 0.0693 - val_accuracy: 0.9785\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0694 - accuracy: 0.9797 - val_loss: 0.0690 - val_accuracy: 0.9785\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9792 - val_loss: 0.0678 - val_accuracy: 0.9785\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0695 - accuracy: 0.9787 - val_loss: 0.0687 - val_accuracy: 0.9785\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9779 - val_loss: 0.0664 - val_accuracy: 0.9800\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0687 - accuracy: 0.9782 - val_loss: 0.0712 - val_accuracy: 0.9800\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9782 - val_loss: 0.0658 - val_accuracy: 0.9800\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9800 - val_loss: 0.0654 - val_accuracy: 0.9808\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0674 - val_accuracy: 0.9792\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9797 - val_loss: 0.0652 - val_accuracy: 0.9800\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0644 - val_accuracy: 0.9808\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9795 - val_loss: 0.0644 - val_accuracy: 0.9808\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0666 - val_accuracy: 0.9792\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.9797 - val_loss: 0.0666 - val_accuracy: 0.9785\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9797 - val_loss: 0.0634 - val_accuracy: 0.9823\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9784 - val_loss: 0.0685 - val_accuracy: 0.9808\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9792 - val_loss: 0.0686 - val_accuracy: 0.9815\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9779 - val_loss: 0.0638 - val_accuracy: 0.9800\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9802 - val_loss: 0.0658 - val_accuracy: 0.9785\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0662 - accuracy: 0.9800 - val_loss: 0.0630 - val_accuracy: 0.9815\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9808 - val_loss: 0.0641 - val_accuracy: 0.9792\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9802 - val_loss: 0.0632 - val_accuracy: 0.9792\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9808 - val_loss: 0.0623 - val_accuracy: 0.9808\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9805 - val_loss: 0.0620 - val_accuracy: 0.9823\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.0624 - val_accuracy: 0.9808\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9797 - val_loss: 0.0625 - val_accuracy: 0.9792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9808 - val_loss: 0.0635 - val_accuracy: 0.9800\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9795 - val_loss: 0.0667 - val_accuracy: 0.9785\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9782 - val_loss: 0.0644 - val_accuracy: 0.9785\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9805 - val_loss: 0.0619 - val_accuracy: 0.9815\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9797 - val_loss: 0.0613 - val_accuracy: 0.9823\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0623 - accuracy: 0.9800 - val_loss: 0.0611 - val_accuracy: 0.9815\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 0.9810 - val_loss: 0.0609 - val_accuracy: 0.9815\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0635 - accuracy: 0.9792 - val_loss: 0.0607 - val_accuracy: 0.9815\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9813 - val_loss: 0.0601 - val_accuracy: 0.9823\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9805 - val_loss: 0.0607 - val_accuracy: 0.9808\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9813 - val_loss: 0.0597 - val_accuracy: 0.9815\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9802 - val_loss: 0.0598 - val_accuracy: 0.9823\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0626 - accuracy: 0.9805 - val_loss: 0.0596 - val_accuracy: 0.9823\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9797 - val_loss: 0.0595 - val_accuracy: 0.9823\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9800 - val_loss: 0.0592 - val_accuracy: 0.9823\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9815 - val_loss: 0.0598 - val_accuracy: 0.9808\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9820 - val_loss: 0.0621 - val_accuracy: 0.9785\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0653 - accuracy: 0.9784 - val_loss: 0.0631 - val_accuracy: 0.9785\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9784 - val_loss: 0.0587 - val_accuracy: 0.9823\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.9800 - val_loss: 0.0622 - val_accuracy: 0.9823\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9813 - val_loss: 0.0592 - val_accuracy: 0.9808\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9815 - val_loss: 0.0591 - val_accuracy: 0.9815\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0601 - accuracy: 0.9808 - val_loss: 0.0583 - val_accuracy: 0.9823\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9813 - val_loss: 0.0592 - val_accuracy: 0.9808\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9805 - val_loss: 0.0595 - val_accuracy: 0.9792\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9815 - val_loss: 0.0577 - val_accuracy: 0.9823\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0674 - accuracy: 0.9790 - val_loss: 0.0612 - val_accuracy: 0.9785\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9802 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9779 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.0662 - val_accuracy: 0.9823\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9828 - val_loss: 0.0579 - val_accuracy: 0.9815\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9802 - val_loss: 0.0593 - val_accuracy: 0.9792\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9810 - val_loss: 0.0577 - val_accuracy: 0.9815\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9808 - val_loss: 0.0574 - val_accuracy: 0.9815\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9820 - val_loss: 0.0567 - val_accuracy: 0.9815\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9810 - val_loss: 0.0575 - val_accuracy: 0.9831\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0672 - accuracy: 0.9802 - val_loss: 0.0574 - val_accuracy: 0.9838\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9808 - val_loss: 0.0574 - val_accuracy: 0.9831\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9818 - val_loss: 0.0571 - val_accuracy: 0.9815\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9815 - val_loss: 0.0626 - val_accuracy: 0.9777\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0611 - accuracy: 0.9813 - val_loss: 0.0618 - val_accuracy: 0.9777\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9823 - val_loss: 0.0601 - val_accuracy: 0.9785\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9823 - val_loss: 0.0613 - val_accuracy: 0.9846\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9810 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9841 - val_loss: 0.0560 - val_accuracy: 0.9831\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9823\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9823\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9826 - val_loss: 0.0554 - val_accuracy: 0.9823\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9836 - val_loss: 0.0557 - val_accuracy: 0.9815\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0554 - val_accuracy: 0.9831\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9843 - val_loss: 0.0562 - val_accuracy: 0.9815\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.0555 - val_accuracy: 0.9831\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9836 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 0.0714 - val_accuracy: 0.9785\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9764 - val_loss: 0.0562 - val_accuracy: 0.9823\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9795 - val_loss: 0.0608 - val_accuracy: 0.9777\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9826 - val_loss: 0.0575 - val_accuracy: 0.9808\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9826 - val_loss: 0.0559 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9831 - val_loss: 0.0559 - val_accuracy: 0.9831\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9836 - val_loss: 0.0578 - val_accuracy: 0.9785\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9843 - val_loss: 0.0543 - val_accuracy: 0.9838\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9828 - val_loss: 0.0544 - val_accuracy: 0.9831\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9838 - val_loss: 0.0559 - val_accuracy: 0.9815\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9843 - val_loss: 0.0551 - val_accuracy: 0.9831\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9841 - val_loss: 0.0574 - val_accuracy: 0.9846\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9826 - val_loss: 0.0588 - val_accuracy: 0.9846\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.0616 - val_accuracy: 0.9846\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9838 - val_loss: 0.0540 - val_accuracy: 0.9823\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9838 - val_loss: 0.0545 - val_accuracy: 0.9823\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0550 - accuracy: 0.9849 - val_loss: 0.0611 - val_accuracy: 0.9769\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9838 - val_loss: 0.0538 - val_accuracy: 0.9823\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9833 - val_loss: 0.0580 - val_accuracy: 0.9854\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9838 - val_loss: 0.0539 - val_accuracy: 0.9823\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0592 - accuracy: 0.9838 - val_loss: 0.0547 - val_accuracy: 0.9823\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9846 - val_loss: 0.0614 - val_accuracy: 0.9785\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.0573 - val_accuracy: 0.9792\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0539 - val_accuracy: 0.9823\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0551 - accuracy: 0.9828 - val_loss: 0.0534 - val_accuracy: 0.9808\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9843 - val_loss: 0.0579 - val_accuracy: 0.9800\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0572 - accuracy: 0.9831 - val_loss: 0.0533 - val_accuracy: 0.9823\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9851 - val_loss: 0.0533 - val_accuracy: 0.9831\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0543 - accuracy: 0.9849 - val_loss: 0.0541 - val_accuracy: 0.9823\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9854 - val_loss: 0.0535 - val_accuracy: 0.9831\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0544 - accuracy: 0.9841 - val_loss: 0.0531 - val_accuracy: 0.9815\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9836 - val_loss: 0.0540 - val_accuracy: 0.9838\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0537 - val_accuracy: 0.9823\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9838 - val_loss: 0.0543 - val_accuracy: 0.9823\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0545 - accuracy: 0.9849 - val_loss: 0.0531 - val_accuracy: 0.9815\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0540 - val_accuracy: 0.9823\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9826 - val_loss: 0.0538 - val_accuracy: 0.9823\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9841 - val_loss: 0.0535 - val_accuracy: 0.9831\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0538 - accuracy: 0.9851 - val_loss: 0.0531 - val_accuracy: 0.9823\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9838 - val_loss: 0.0542 - val_accuracy: 0.9838\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0544 - val_accuracy: 0.9838\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9849 - val_loss: 0.0530 - val_accuracy: 0.9831\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 679us/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.0567 - val_accuracy: 0.9808\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 0.9851 - val_loss: 0.0532 - val_accuracy: 0.9831\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9836 - val_loss: 0.0526 - val_accuracy: 0.9815\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9846 - val_loss: 0.0551 - val_accuracy: 0.9854\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 468us/step - loss: 0.0541 - accuracy: 0.9854 - val_loss: 0.0528 - val_accuracy: 0.9831\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9849 - val_loss: 0.0529 - val_accuracy: 0.9831\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 389us/step - loss: 0.0528 - accuracy: 0.9846 - val_loss: 0.0527 - val_accuracy: 0.9815\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9843 - val_loss: 0.0529 - val_accuracy: 0.9838\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0527 - accuracy: 0.9846 - val_loss: 0.0525 - val_accuracy: 0.9823\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9843 - val_loss: 0.0529 - val_accuracy: 0.9831\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.9846 - val_loss: 0.0535 - val_accuracy: 0.9823\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9856 - val_loss: 0.0528 - val_accuracy: 0.9831\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9846 - val_loss: 0.0539 - val_accuracy: 0.9838\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9851 - val_loss: 0.0526 - val_accuracy: 0.9815\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9851 - val_loss: 0.0531 - val_accuracy: 0.9815\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9843 - val_loss: 0.0525 - val_accuracy: 0.9831\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 0s 584us/step - loss: 0.0519 - accuracy: 0.9846 - val_loss: 0.0537 - val_accuracy: 0.9823\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9851 - val_loss: 0.0527 - val_accuracy: 0.9846\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0532 - accuracy: 0.9841 - val_loss: 0.0539 - val_accuracy: 0.9823\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9859 - val_loss: 0.0542 - val_accuracy: 0.9846\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9854 - val_loss: 0.0527 - val_accuracy: 0.9831\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0519 - accuracy: 0.9843 - val_loss: 0.0522 - val_accuracy: 0.9815\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0518 - accuracy: 0.9849 - val_loss: 0.0523 - val_accuracy: 0.9831\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0523 - val_accuracy: 0.9831\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9846 - val_loss: 0.0527 - val_accuracy: 0.9831\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9849 - val_loss: 0.0524 - val_accuracy: 0.9823\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9843 - val_loss: 0.0558 - val_accuracy: 0.9808\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0526 - accuracy: 0.9843 - val_loss: 0.0523 - val_accuracy: 0.9823\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9838 - val_loss: 0.0526 - val_accuracy: 0.9823\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0519 - accuracy: 0.9846 - val_loss: 0.0520 - val_accuracy: 0.9831\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0523 - accuracy: 0.9843 - val_loss: 0.0525 - val_accuracy: 0.9823\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.0522 - val_accuracy: 0.9815\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9849 - val_loss: 0.0520 - val_accuracy: 0.9823\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9828 - val_loss: 0.0521 - val_accuracy: 0.9823\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.0569 - val_accuracy: 0.9854\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9836 - val_loss: 0.0548 - val_accuracy: 0.9854\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9846 - val_loss: 0.0562 - val_accuracy: 0.9808\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9838 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9841 - val_loss: 0.0521 - val_accuracy: 0.9815\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.0521 - val_accuracy: 0.9823\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9836 - val_loss: 0.0563 - val_accuracy: 0.9854\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9838 - val_loss: 0.0528 - val_accuracy: 0.9838\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9841 - val_loss: 0.0539 - val_accuracy: 0.9846\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0516 - accuracy: 0.9856 - val_loss: 0.0559 - val_accuracy: 0.9854\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0522 - val_accuracy: 0.9838\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 584us/step - loss: 0.0581 - accuracy: 0.9820 - val_loss: 0.0521 - val_accuracy: 0.9815\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 377us/step - loss: 0.0518 - accuracy: 0.9831 - val_loss: 0.0564 - val_accuracy: 0.9800\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.0524 - val_accuracy: 0.9846\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9843 - val_loss: 0.0522 - val_accuracy: 0.9838\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0506 - accuracy: 0.9867 - val_loss: 0.0544 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c862835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 557us/step - loss: 0.0531 - accuracy: 0.9815\n",
      "Test accuracy: 0.9815384745597839\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1798ad1",
   "metadata": {},
   "source": [
    "위와 같이 진행 하고 나면 학습모델만 불러와서 학습 시키는게 좋다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
